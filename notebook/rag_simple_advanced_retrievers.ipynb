{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read PDFs from S3 bucket using PyPDFLoader\n",
    "import boto3\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles loading and processing of documents from S3 using PyPDFLoader.\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, prefix: str = \"pdfs/\"):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_documents(self) -> List[Any]:\n",
    "        \"\"\"Load all PDF documents from S3 bucket using PyPDFLoader.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading PDFs from s3://{self.bucket_name}/{self.prefix}\")\n",
    "            \n",
    "            # List all PDF files in S3 bucket\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.bucket_name,\n",
    "                Prefix=self.prefix\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                print(f\"No files found in s3://{self.bucket_name}/{self.prefix}\")\n",
    "                return []\n",
    "            \n",
    "            # Filter PDF files\n",
    "            pdf_files = [obj['Key'] for obj in response['Contents'] \n",
    "                        if obj['Key'].endswith('.pdf')]\n",
    "            \n",
    "            print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "            \n",
    "            # Load each PDF file\n",
    "            all_documents = []\n",
    "            for pdf_key in pdf_files:\n",
    "                print(f\"Processing: {pdf_key}\")\n",
    "                \n",
    "                # Download PDF to temporary file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "                    tmp_path = tmp_file.name\n",
    "                    self.s3_client.download_file(self.bucket_name, pdf_key, tmp_path)\n",
    "                \n",
    "                try:\n",
    "                    # Load PDF using PyPDFLoader\n",
    "                    loader = PyPDFLoader(tmp_path)\n",
    "                    documents = loader.load()\n",
    "                    \n",
    "                    # Add source information to metadata\n",
    "                    for doc in documents:\n",
    "                        doc.metadata['s3_bucket'] = self.bucket_name\n",
    "                        doc.metadata['s3_key'] = pdf_key\n",
    "                        doc.metadata['source_file'] = pdf_key.split('/')[-1]\n",
    "                    \n",
    "                    all_documents.extend(documents)\n",
    "                    \n",
    "                    print(f\"  - Loaded {len(documents)} page(s)\")\n",
    "                    print(f\"  - Total characters: {sum(len(doc.page_content) for doc in documents)}\")\n",
    "                    \n",
    "                finally:\n",
    "                    # Clean up temporary file\n",
    "                    if os.path.exists(tmp_path):\n",
    "                        os.remove(tmp_path)\n",
    "            \n",
    "            self.documents = all_documents\n",
    "            print(f\"\\nTotal documents loaded: {len(self.documents)} pages from {len(pdf_files)} PDF(s)\")\n",
    "            print(f\"Total characters: {sum(len(doc.page_content) for doc in self.documents)}\")\n",
    "            \n",
    "            if self.documents:\n",
    "                print(f\"\\nFirst document metadata: {self.documents[0].metadata}\")\n",
    "                print(f\"First page preview: {self.documents[0].page_content[:150]}...\")\n",
    "            \n",
    "            return self.documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def split_documents_into_chunks(self, chunk_size: int = 500, chunk_overlap: int = 50) -> List[Any]:\n",
    "        \"\"\"Split loaded documents into smaller chunks with overlap.\"\"\"\n",
    "        try:\n",
    "            if not self.documents:\n",
    "                raise ValueError(\"No documents loaded. Call load_documents() first.\")\n",
    "            \n",
    "            print(f\"\\nSplitting documents into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "            \n",
    "            # Use RecursiveCharacterTextSplitter for better chunking\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            chunks = text_splitter.split_documents(self.documents)\n",
    "            \n",
    "            print(f\"Split into {len(chunks)} chunks\")\n",
    "            print(f\"Average chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "            \n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting documents: {e}\")\n",
    "            raise\n",
    "\n",
    "# Initialize and use the DocumentProcessor\n",
    "doc_processor = DocumentProcessor(bucket_name=\"test\", prefix=\"pdfs/\")\n",
    "\n",
    "# Load all documents\n",
    "all_documents = doc_processor.load_documents()\n",
    "\n",
    "# Split into chunks\n",
    "document_chunks = doc_processor.split_documents_into_chunks(chunk_size=1500, chunk_overlap=50)\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439cd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,    Dict,   Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model.\"\"\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Loaded model: {self.model_name}\")\n",
    "            print(\"Embedding dimension:\", self.model.get_sentence_embedding_dimension() )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True )\n",
    "        print(f\"Generated embeddings for {len(texts)} texts.\")\n",
    "        print(f\"Embedding shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def get_sentence_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the dimension of the sentence embeddings.\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "\n",
    "## initalize embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager(model_name='all-MiniLM-L6-v2')\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c086c3",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6279e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\" Manages a vector store using ChromaDB.\"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persistent_directory: str = \"../data/vector_store\"):\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection.\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persistent_directory)\n",
    "\n",
    "            # Get or Create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG system\"}                   \n",
    "            )\n",
    "            print(f\"Vector store initialized: {self.collection_name} at {self.persistent_directory}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Vector Store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store.\"\"\"\n",
    "        try:\n",
    "\n",
    "            if len(documents) != len(embeddings):\n",
    "                raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "            \n",
    "            print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "            # Prepare data for ChromaDB\n",
    "            ids = []\n",
    "            metadatas = []\n",
    "            documents_texts = []\n",
    "            embeddings_list = []\n",
    "\n",
    "            for i, (doc, emb) in enumerate(zip(documents, embeddings)):\n",
    "                # Generate a unique ID for each document\n",
    "                doc_id = str(uuid.uuid4().hex[:8])\n",
    "                ids.append(doc_id)\n",
    "\n",
    "                # Prepare metadata\n",
    "                metadata = dict(doc.metadata)\n",
    "                metadata['doc_index'] = i\n",
    "                metadata['content_length'] = len(doc.page_content)\n",
    "                metadatas.append(metadata)\n",
    "                \n",
    "                # Document Content\n",
    "                documents_texts.append(doc.page_content)\n",
    "                # Embeddings\n",
    "                embeddings_list.append(emb.tolist())\n",
    "\n",
    "            # Add to ChromaDB collection\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_texts\n",
    "            )\n",
    "\n",
    "            print(f\"Added {len(documents)} documents to the vector store.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to Vector Store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorStore = VectorStore(collection_name=\"pdf_documents\", persistent_directory=\"../data/vector_store\")\n",
    "vectorStore\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert document chunks to embeddings\n",
    "texts = [doc.page_content for doc in document_chunks]\n",
    "\n",
    "### Generate embeddings for document chunks\n",
    "embeddings = embedding_manager.generate_embedding(texts)\n",
    "\n",
    "### Store in the Vector Store\n",
    "vectorStore.add_documents(document_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783aac8",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Augmented Generation- Retrieval Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dac0b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x129db12b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Retriever for RAG system using ChromaDB vector store.\"\"\"\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\" Intialize the retriever with vector store and embedding manager.\"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.4) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve top_k relevant documents for the given query.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n=== Retrieving documents for query ===\")\n",
    "            print(f\"Query: '{query}'\")\n",
    "            print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "            \n",
    "            # Generate embedding for the query - First Convert query to Embedding\n",
    "            print(\"Generating query embedding...\")\n",
    "            query_embeddings = self.embedding_manager.generate_embedding([query])\n",
    "            query_embedding = query_embeddings[0].tolist()\n",
    "            print(f\"Query embedding shape: {len(query_embedding)}\")\n",
    "\n",
    "            # Query the vector store\n",
    "            print(\"\\nQuerying vector store...\")\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            print(f\"\\nQuery returned {len(results['ids'][0])} results\")\n",
    "\n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and len(results['documents'][0]) > 0:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                print(\"\\n=== Results ===\")\n",
    "                for i, (doc_id, doc, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score\n",
    "                    # ChromaDB uses L2 distance, so lower is better\n",
    "                    similarity_score = 1 / (1 + distance)  # Convert distance to similarity\n",
    "                    \n",
    "                    print(f\"\\nRank {i+1}:\")\n",
    "                    print(f\"  Doc ID: {doc_id}\")\n",
    "                    print(f\"  Distance: {distance:.4f}\")\n",
    "                    print(f\"  Similarity Score: {similarity_score:.4f}\")\n",
    "                    print(f\"  Source: {metadata.get('source_file', 'Unknown')}\")\n",
    "                    print(f\"  Page: {metadata.get('page', 'N/A')}\")\n",
    "                    print(f\"  Content preview: {doc[:150]}...\")\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        doc_info = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"content\": doc,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            \"distance\": distance,\n",
    "                            \"rank\": i + 1\n",
    "                        }\n",
    "                        retrieved_docs.append(doc_info)\n",
    "                        \n",
    "                print(f\"\\n=== Summary ===\")\n",
    "                print(f\"Documents above similarity threshold ({score_threshold}): {len(retrieved_docs)}\")\n",
    "            else:\n",
    "                print(\"No documents retrieved.\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vectorStore, embedding_manager=embedding_manager)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(query=\"describe about Attention is All you Need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug: Check vector store contents and search\n",
    "print(\"=== Vector Store Debug Info ===\")\n",
    "print(f\"Total documents in vector store: {vectorStore.collection.count()}\")\n",
    "\n",
    "# Check if the Attention paper is in the documents\n",
    "print(\"\\n=== Checking for 'Attention' paper ===\")\n",
    "attention_chunks = [doc for doc in document_chunks if 'attention' in doc.page_content.lower() \n",
    "                    or 'attention' in doc.metadata.get('source_file', '').lower()]\n",
    "print(f\"Found {len(attention_chunks)} chunks mentioning 'attention'\")\n",
    "\n",
    "if attention_chunks:\n",
    "    print(f\"\\nFirst attention chunk metadata: {attention_chunks[0].metadata}\")\n",
    "    print(f\"First attention chunk preview: {attention_chunks[0].page_content[:200]}...\")\n",
    "\n",
    "# Test the query with debug info\n",
    "print(\"\\n=== Testing Query ===\")\n",
    "query = \"describe about Attention is All you Need\"\n",
    "print(f\"Query: {query}\")\n",
    "results = rag_retriever.retrieve(query=query, top_k=5, score_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa97f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if documents were actually added to vector store\n",
    "print(\"=== Verifying Vector Store Population ===\")\n",
    "print(f\"Number of chunks created: {len(document_chunks)}\")\n",
    "print(f\"Number of embeddings generated: {embeddings.shape[0] if embeddings is not None else 0}\")\n",
    "print(f\"Documents in vector store: {vectorStore.collection.count()}\")\n",
    "\n",
    "# Sample a random query from the vector store\n",
    "sample_result = vectorStore.collection.query(\n",
    "    query_embeddings=[embeddings[0].tolist()],\n",
    "    n_results=3\n",
    ")\n",
    "print(f\"\\nSample query returned {len(sample_result['ids'][0])} results\")\n",
    "if sample_result['documents']:\n",
    "    print(f\"Sample document preview: {sample_result['documents'][0][0][:200]}...\")\n",
    "    print(f\"Sample distances: {sample_result['distances'][0]}\")\n",
    "    print(f\"Sample metadata: {sample_result['metadatas'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24453851",
   "metadata": {},
   "source": [
    "### RAG with Groq LLM Simple \n",
    "\n",
    "Implementing complete RAG pipeline with Groq open source LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple RAG pipeline with Groq LLM\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "### Initialize Groq LLM with API key from environment variable\n",
    "groq_api_key = \"<APIKEY OF GROQ>\"\n",
    "\n",
    "llm = ChatGroq(api_key=groq_api_key, model=\"llama-3.1-8b-instant\", temperature=0.2, max_tokens=1000)\n",
    "\n",
    "## Simple RAG function: retrieve context +  generate response\n",
    "\n",
    "def rag_simple(query,retriever:RAGRetriever, llm:ChatGroq, top_k:int=5, score_threshold:float=0.5) -> str:\n",
    "    \"\"\"Simple RAG pipeline: retrieve context and generate answer.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.retrieve(query=query, top_k=top_k, score_threshold=score_threshold)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"I cannot find the answer in the provided documents.\"\n",
    "    \n",
    "    # Combine retrieved document contents\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"I cannot find the answer in the provided documents.\"\n",
    "    \n",
    "    # Create prompt for LLM\n",
    "    prompt = f\"\"\"Answer the following question based on the provided context. If the answer cannot be found in the context, say \"I cannot find the answer in the provided documents.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer using LLM\n",
    "    response = llm.invoke(prompt.format(query=query, context=context))\n",
    "    \n",
    "    return response.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer= rag_simple(query=\"Explain Model Architecture of Attention is all you Need Paper\", retriever=rag_retriever, llm=llm, top_k=3, score_threshold=0.3)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f381a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query: str, retriever: RAGRetriever, llm: ChatGroq, top_k: int = 5, score_threshold: float = 0.5,return_context: bool = False) -> str:\n",
    "    \"\"\"RAG pipeline with extra features\n",
    "        Returns answer,sources,confidence score and optionally full context\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.retrieve(query=query, top_k=top_k, score_threshold=score_threshold)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"I cannot find the answer in the provided documents.\"\n",
    "    \n",
    "    # Combine retrieved document contents\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"I cannot find the answer in the provided documents.\"\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    sources = [{\n",
    "            'source': doc['metadata'].get('source_file', 'Unknown'),\n",
    "            'similarity_score': doc['similarity_score'],\n",
    "            'rank': doc['rank'],\n",
    "            'page': doc['metadata'].get('page', 'N/A'),\n",
    "            'preview': doc['content'][:150]\n",
    "        } for doc in retrieved_docs\n",
    "    ]\n",
    "    confidence_score = np.mean([doc['similarity_score'] for doc in retrieved_docs])\n",
    "\n",
    "\n",
    "    # Generate Answer using LLM\n",
    "\n",
    "    prompt = f\"\"\"Answer the following question based on the provided context. If the answer cannot be found in the context, say \"I cannot find the answer in the provided documents.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer using LLM\n",
    "    response = llm.invoke([prompt.format(query=query, context=context)])\n",
    "\n",
    "    output={\n",
    "        'answer': response.content.strip(),\n",
    "        'sources': sources,\n",
    "        'confidence_score': confidence_score,\n",
    "        'full_context': context\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output['context']= context\n",
    "    return output    \n",
    "\n",
    "    # Example usage\n",
    "result = rag_advanced(query=\"Explain Model Architecture of Attention is all you Need Paper\", retriever=rag_retriever, llm=llm, top_k=3, score_threshold=0.3)\n",
    "result   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
