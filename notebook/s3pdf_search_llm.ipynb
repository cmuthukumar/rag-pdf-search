{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read PDFs from S3 bucket using PyPDFLoader\n",
    "import boto3\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles loading and processing of documents from S3 using PyPDFLoader.\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, prefix: str = \"pdfs/\"):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_documents(self) -> List[Any]:\n",
    "        \"\"\"Load all PDF documents from S3 bucket using PyPDFLoader.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading PDFs from s3://{self.bucket_name}/{self.prefix}\")\n",
    "            \n",
    "            # List all PDF files in S3 bucket\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.bucket_name,\n",
    "                Prefix=self.prefix\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                print(f\"No files found in s3://{self.bucket_name}/{self.prefix}\")\n",
    "                return []\n",
    "            \n",
    "            # Filter PDF files\n",
    "            pdf_files = [obj['Key'] for obj in response['Contents'] \n",
    "                        if obj['Key'].endswith('.pdf')]\n",
    "            \n",
    "            print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "            \n",
    "            # Load each PDF file\n",
    "            all_documents = []\n",
    "            for pdf_key in pdf_files:\n",
    "                print(f\"Processing: {pdf_key}\")\n",
    "                \n",
    "                # Download PDF to temporary file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "                    tmp_path = tmp_file.name\n",
    "                    self.s3_client.download_file(self.bucket_name, pdf_key, tmp_path)\n",
    "                \n",
    "                try:\n",
    "                    # Load PDF using PyPDFLoader\n",
    "                    loader = PyPDFLoader(tmp_path)\n",
    "                    documents = loader.load()\n",
    "                    \n",
    "                    # Add source information to metadata\n",
    "                    for doc in documents:\n",
    "                        doc.metadata['s3_bucket'] = self.bucket_name\n",
    "                        doc.metadata['s3_key'] = pdf_key\n",
    "                        doc.metadata['source_file'] = pdf_key.split('/')[-1]\n",
    "                    \n",
    "                    all_documents.extend(documents)\n",
    "                    \n",
    "                    print(f\"  - Loaded {len(documents)} page(s)\")\n",
    "                    print(f\"  - Total characters: {sum(len(doc.page_content) for doc in documents)}\")\n",
    "                    \n",
    "                finally:\n",
    "                    # Clean up temporary file\n",
    "                    if os.path.exists(tmp_path):\n",
    "                        os.remove(tmp_path)\n",
    "            \n",
    "            self.documents = all_documents\n",
    "            print(f\"\\nTotal documents loaded: {len(self.documents)} pages from {len(pdf_files)} PDF(s)\")\n",
    "            print(f\"Total characters: {sum(len(doc.page_content) for doc in self.documents)}\")\n",
    "            \n",
    "            if self.documents:\n",
    "                print(f\"\\nFirst document metadata: {self.documents[0].metadata}\")\n",
    "                print(f\"First page preview: {self.documents[0].page_content[:150]}...\")\n",
    "            \n",
    "            return self.documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def split_documents_into_chunks(self, chunk_size: int = 500, chunk_overlap: int = 50) -> List[Any]:\n",
    "        \"\"\"Split loaded documents into smaller chunks with overlap.\"\"\"\n",
    "        try:\n",
    "            if not self.documents:\n",
    "                raise ValueError(\"No documents loaded. Call load_documents() first.\")\n",
    "            \n",
    "            print(f\"\\nSplitting documents into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "            \n",
    "            # Use RecursiveCharacterTextSplitter for better chunking\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            chunks = text_splitter.split_documents(self.documents)\n",
    "            \n",
    "            print(f\"Split into {len(chunks)} chunks\")\n",
    "            print(f\"Average chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "            \n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting documents: {e}\")\n",
    "            raise\n",
    "\n",
    "# Initialize and use the DocumentProcessor\n",
    "doc_processor = DocumentProcessor(bucket_name=\"test\", prefix=\"pdfs/\")\n",
    "\n",
    "# Load all documents\n",
    "all_documents = doc_processor.load_documents()\n",
    "\n",
    "# Split into chunks\n",
    "document_chunks = doc_processor.split_documents_into_chunks(chunk_size=1500, chunk_overlap=50)\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439cd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,    Dict,   Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model.\"\"\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Loaded model: {self.model_name}\")\n",
    "            print(\"Embedding dimension:\", self.model.get_sentence_embedding_dimension() )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True )\n",
    "        print(f\"Generated embeddings for {len(texts)} texts.\")\n",
    "        print(f\"Embedding shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def get_sentence_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the dimension of the sentence embeddings.\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "\n",
    "## initalize embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager(model_name='all-MiniLM-L6-v2')\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c086c3",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6279e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\" Manages a vector store using ChromaDB.\"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persistent_directory: str = \"../data/vector_store\"):\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection.\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persistent_directory)\n",
    "\n",
    "            # Get or Create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG system\"}                   \n",
    "            )\n",
    "            print(f\"Vector store initialized: {self.collection_name} at {self.persistent_directory}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Vector Store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store.\"\"\"\n",
    "        try:\n",
    "\n",
    "            if len(documents) != len(embeddings):\n",
    "                raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "            \n",
    "            print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "            # Prepare data for ChromaDB\n",
    "            ids = []\n",
    "            metadatas = []\n",
    "            documents_texts = []\n",
    "            embeddings_list = []\n",
    "\n",
    "            for i, (doc, emb) in enumerate(zip(documents, embeddings)):\n",
    "                # Generate a unique ID for each document\n",
    "                doc_id = str(uuid.uuid4().hex[:8])\n",
    "                ids.append(doc_id)\n",
    "\n",
    "                # Prepare metadata\n",
    "                metadata = dict(doc.metadata)\n",
    "                metadata['doc_index'] = i\n",
    "                metadata['content_length'] = len(doc.page_content)\n",
    "                metadatas.append(metadata)\n",
    "                \n",
    "                # Document Content\n",
    "                documents_texts.append(doc.page_content)\n",
    "                # Embeddings\n",
    "                embeddings_list.append(emb.tolist())\n",
    "\n",
    "            # Add to ChromaDB collection\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_texts\n",
    "            )\n",
    "\n",
    "            print(f\"Added {len(documents)} documents to the vector store.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to Vector Store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorStore = VectorStore(collection_name=\"pdf_documents\", persistent_directory=\"../data/vector_store\")\n",
    "vectorStore\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7052e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 55 texts.\n",
      "Embedding shape: (55, 384)\n",
      "Adding 55 documents to the vector store...\n",
      "Added 55 documents to the vector store.\n"
     ]
    }
   ],
   "source": [
    "### Convert document chunks to embeddings\n",
    "texts = [doc.page_content for doc in document_chunks]\n",
    "\n",
    "### Generate embeddings for document chunks\n",
    "embeddings = embedding_manager.generate_embedding(texts)\n",
    "\n",
    "### Store in the Vector Store\n",
    "vectorStore.add_documents(document_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783aac8",
   "metadata": {},
   "source": [
    "### RAG - Retrieval Augmented Generation- Retrieval Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dac0b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x125d54050>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Retriever for RAG system using ChromaDB vector store.\"\"\"\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\" Intialize the retriever with vector store and embedding manager.\"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.4) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve top_k relevant documents for the given query.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n=== Retrieving documents for query ===\")\n",
    "            print(f\"Query: '{query}'\")\n",
    "            print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "            \n",
    "            # Generate embedding for the query - First Convert query to Embedding\n",
    "            print(\"Generating query embedding...\")\n",
    "            query_embeddings = self.embedding_manager.generate_embedding([query])\n",
    "            query_embedding = query_embeddings[0].tolist()\n",
    "            print(f\"Query embedding shape: {len(query_embedding)}\")\n",
    "\n",
    "            # Query the vector store\n",
    "            print(\"\\nQuerying vector store...\")\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            print(f\"\\nQuery returned {len(results['ids'][0])} results\")\n",
    "\n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and len(results['documents'][0]) > 0:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                print(\"\\n=== Results ===\")\n",
    "                for i, (doc_id, doc, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score\n",
    "                    # ChromaDB uses L2 distance, so lower is better\n",
    "                    similarity_score = 1 / (1 + distance)  # Convert distance to similarity\n",
    "                    \n",
    "                    print(f\"\\nRank {i+1}:\")\n",
    "                    print(f\"  Doc ID: {doc_id}\")\n",
    "                    print(f\"  Distance: {distance:.4f}\")\n",
    "                    print(f\"  Similarity Score: {similarity_score:.4f}\")\n",
    "                    print(f\"  Source: {metadata.get('source_file', 'Unknown')}\")\n",
    "                    print(f\"  Page: {metadata.get('page', 'N/A')}\")\n",
    "                    print(f\"  Content preview: {doc[:150]}...\")\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        doc_info = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"content\": doc,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            \"distance\": distance,\n",
    "                            \"rank\": i + 1\n",
    "                        }\n",
    "                        retrieved_docs.append(doc_info)\n",
    "                        \n",
    "                print(f\"\\n=== Summary ===\")\n",
    "                print(f\"Documents above similarity threshold ({score_threshold}): {len(retrieved_docs)}\")\n",
    "            else:\n",
    "                print(\"No documents retrieved.\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vectorStore, embedding_manager=embedding_manager)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(query=\"describe about Attention is All you Need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b6755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug: Check vector store contents and search\n",
    "print(\"=== Vector Store Debug Info ===\")\n",
    "print(f\"Total documents in vector store: {vectorStore.collection.count()}\")\n",
    "\n",
    "# Check if the Attention paper is in the documents\n",
    "print(\"\\n=== Checking for 'Attention' paper ===\")\n",
    "attention_chunks = [doc for doc in document_chunks if 'attention' in doc.page_content.lower() \n",
    "                    or 'attention' in doc.metadata.get('source_file', '').lower()]\n",
    "print(f\"Found {len(attention_chunks)} chunks mentioning 'attention'\")\n",
    "\n",
    "if attention_chunks:\n",
    "    print(f\"\\nFirst attention chunk metadata: {attention_chunks[0].metadata}\")\n",
    "    print(f\"First attention chunk preview: {attention_chunks[0].page_content[:200]}...\")\n",
    "\n",
    "# Test the query with debug info\n",
    "print(\"\\n=== Testing Query ===\")\n",
    "query = \"describe about Attention is All you Need\"\n",
    "print(f\"Query: {query}\")\n",
    "results = rag_retriever.retrieve(query=query, top_k=5, score_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa97f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if documents were actually added to vector store\n",
    "print(\"=== Verifying Vector Store Population ===\")\n",
    "print(f\"Number of chunks created: {len(document_chunks)}\")\n",
    "print(f\"Number of embeddings generated: {embeddings.shape[0] if embeddings is not None else 0}\")\n",
    "print(f\"Documents in vector store: {vectorStore.collection.count()}\")\n",
    "\n",
    "# Sample a random query from the vector store\n",
    "sample_result = vectorStore.collection.query(\n",
    "    query_embeddings=[embeddings[0].tolist()],\n",
    "    n_results=3\n",
    ")\n",
    "print(f\"\\nSample query returned {len(sample_result['ids'][0])} results\")\n",
    "if sample_result['documents']:\n",
    "    print(f\"Sample document preview: {sample_result['documents'][0][0][:200]}...\")\n",
    "    print(f\"Sample distances: {sample_result['distances'][0]}\")\n",
    "    print(f\"Sample metadata: {sample_result['metadatas'][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24453851",
   "metadata": {},
   "source": [
    "### RAG with Open Source LLM (Hugging Face)\n",
    "\n",
    "Implementing complete RAG pipeline with Hugging Face's open source LLM for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if not already installed\n",
    "# !pip install transformers torch\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class RAGWithLLM:\n",
    "    \"\"\"Complete RAG system with Hugging Face LLM for question answering.\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever: RAGRetriever, model_name: str = \"google/flan-t5-base\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG with LLM.\n",
    "        \n",
    "        Args:\n",
    "            retriever: RAGRetriever instance for document retrieval\n",
    "            model_name: Hugging Face model name (default: google/flan-t5-base)\n",
    "                       Options: \n",
    "                       - \"google/flan-t5-small\" (fast, 80M params)\n",
    "                       - \"google/flan-t5-base\" (balanced, 250M params)\n",
    "                       - \"google/flan-t5-large\" (better quality, 780M params)\n",
    "        \"\"\"\n",
    "        self.retriever = retriever\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Initializing RAG with LLM: {model_name}\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the LLM model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            print(\"Loading model (this may take a moment)...\")\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None\n",
    "            )\n",
    "            \n",
    "            if self.device == \"cpu\":\n",
    "                self.model = self.model.to(self.device)\n",
    "            \n",
    "            print(f\"✓ Model loaded successfully on {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create a prompt for the LLM with query and context.\"\"\"\n",
    "        prompt = f\"\"\"Answer the following question based on the provided context. If the answer cannot be found in the context, say \"I cannot find the answer in the provided documents.\"\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {query}\n",
    "\n",
    "            Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_answer(\n",
    "        self, \n",
    "        query: str, \n",
    "        top_k: int = 3,\n",
    "        score_threshold: float = 0.4,\n",
    "        max_length: int = 256\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generate answer using RAG pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            top_k: Number of documents to retrieve\n",
    "            score_threshold: Minimum similarity score for retrieval\n",
    "            max_length: Maximum length of generated answer\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, sources, and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"QUERY: {query}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Step 1: Retrieve relevant documents\n",
    "            print(\"\\n[1/3] Retrieving relevant documents...\")\n",
    "            retrieved_docs = self.retriever.retrieve(\n",
    "                query=query,\n",
    "                top_k=top_k,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"answer\": \"No relevant documents found to answer this question.\",\n",
    "                    \"sources\": [],\n",
    "                    \"num_sources\": 0\n",
    "                }\n",
    "            \n",
    "            # Step 2: Prepare context from retrieved documents\n",
    "            print(f\"\\n[2/3] Preparing context from {len(retrieved_docs)} documents...\")\n",
    "            context_parts = []\n",
    "            for i, doc in enumerate(retrieved_docs, 1):\n",
    "                context_parts.append(\n",
    "                    f\"[Document {i} - {doc['metadata'].get('source_file', 'Unknown')}]:\\n{doc['content']}\"\n",
    "                )\n",
    "            \n",
    "            context = \"\\n\\n\".join(context_parts)\n",
    "            \n",
    "            # Step 3: Generate answer using LLM\n",
    "            print(\"\\n[3/3] Generating answer with LLM...\")\n",
    "            prompt = self._create_prompt(query, context)\n",
    "            \n",
    "            # Generate answer\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=4,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "            \n",
    "            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Prepare sources information\n",
    "            sources = [\n",
    "                {\n",
    "                    \"source_file\": doc['metadata'].get('source_file', 'Unknown'),\n",
    "                    \"page\": doc['metadata'].get('page', 'N/A'),\n",
    "                    \"similarity_score\": doc['similarity_score'],\n",
    "                    \"content_preview\": doc['content'][:200] + \"...\"\n",
    "                }\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"ANSWER:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(answer)\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"SOURCES ({len(sources)} documents):\")\n",
    "            print(f\"{'='*60}\")\n",
    "            for i, source in enumerate(sources, 1):\n",
    "                print(f\"\\n{i}. {source['source_file']} (Page {source['page']})\")\n",
    "                print(f\"   Similarity: {source['similarity_score']:.4f}\")\n",
    "                print(f\"   Preview: {source['content_preview'][:150]}...\")\n",
    "            \n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"num_sources\": len(sources)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"num_sources\": 0\n",
    "            }\n",
    "\n",
    "# Initialize RAG with LLM (using lightweight FLAN-T5 model)\n",
    "print(\"Initializing RAG with Open Source LLM...\")\n",
    "rag_llm = RAGWithLLM(\n",
    "    retriever=rag_retriever,\n",
    "    model_name=\"google/flan-t5-base\"  # Change to \"google/flan-t5-small\" for faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbff060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ask a question about your documents\n",
    "result = rag_llm.generate_answer(\n",
    "    query=\"Describe about attention mechanism?\",\n",
    "    top_k=3,\n",
    "    score_threshold=0.3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULT:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Question: {result['query']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Sources used: {result['num_sources']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
